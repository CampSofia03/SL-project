#---
#  Title: Logistic Regression Model & Linear Discriminant Analiysis
#  Author: Campregher Sofia
# Source: https://www.kaggle.com/datasets/spscientist/students-performance-in-exams/data

#---


###
#1 - Data Preparation 
###

file_path <- file.choose()
data <- read.csv(file_path)

# 1.a - Outlier elimination 

numeric_cols <- c("math.score", "reading.score", "writing.score")

summary(data)

for (col in numeric_cols) {
  Q1 <- quantile(data[[col]], 0.25, na.rm = TRUE)
  Q3 <- quantile(data[[col]], 0.75, na.rm = TRUE)
  IQR_value <- Q3 - Q1
  
  cat("\nColumn:", col, "\nQ1:", Q1, "\nQ3:", Q3, "\nIQR:", IQR_value, "\n")
  cat("Pre-row:", nrow(data), "\n")
  
  data <- data[data[[col]] >= (Q1 - 1.5 * IQR_value) & 
                 data[[col]] <= (Q3 + 1.5 * IQR_value), ]
  
  cat("Post-row", nrow(data), "\n")
}

summary(data)


###
#1 - Logistic regression Analiysis
###


# 1.a - Logistic regression model

if (!requireNamespace("dplyr", quietly = TRUE)) {install.packages("dplyr")}
library(dplyr)
if (!requireNamespace("ggplot2", quietly = TRUE)) {install.packages("ggplot2")}
library(ggplot2)
par(mfrow = c(1,1))
par(mar = c(5, 4, 4, 2) + 0.1)

student <- data %>% mutate(pass = ifelse(math.score > 60,1,0) %>% as.factor())
head(student)

set.seed(1)
train <- sample(1:nrow(student),nrow(student)*.9,rep = F)
lr_model <- glm(pass ~ . -math.score - reading.score - writing.score,data = student, family = binomial,subset = train)
summary(lr_model)

# 1.b - Evaluation of predictor vectors

# Confusion
probabilities <- predict(lr_model, newdata = student[-train, ], type = "response")
predictions <- ifelse(probabilities > 0.5, 1, 0)
confusion_matrix <- table(Predicted = predictions, Actual = student$pass[-train])
print(confusion_matrix)

# Accuracy
test <- setdiff(1:nrow(student), train)
pred <- predict(lr_model, newdata = student[test, ], type = "response")
pred_class <- ifelse(pred > 0.5, 1, 0)
mean(pred_class == student$pass[test])

# Precision
precision <- confusion_matrix[2,2] / sum(confusion_matrix[2,])
precision

# Recall
recall <- confusion_matrix[2,2] / sum(confusion_matrix[,2])
recall

# F1
F1_score <- 2 * (precision * recall) / (precision + recall)
F1_score

# Specificity
specificity <- confusion_matrix[1,1] / sum(confusion_matrix[,1])
specificity

# Misclassification rate
misclascification_rate <- mean(predictions != student$pass[-train])
cat("Misclasification Rate:", misclascification_rate, "\n")

# False positive rate
false_positive_rate <- confusion_matrix[2,1] / sum(confusion_matrix[,1])
cat("False Positive Rate:", false_positive_rate, "\n")

# False negative rate
false_negative_rate <- confusion_matrix[1,2] / sum(confusion_matrix[,2])
cat("False Negative Rate:", false_negative_rate, "\n")

# Probability distribution
ggplot(data.frame(Probability = probabilities, Pass = as.factor(student$pass[-train])),
       aes(x = Probability, fill = Pass)) +
  geom_histogram(alpha = 0.6, bins = 30, position = "identity") +
  labs(title = "Probability distribution - LR",
       x = "Predicted Probability",
       y = "Frequency") +
  theme_minimal()

# ROC - AUC
if (!requireNamespace("pROC", quietly = TRUE)) {install.packages("pROC")}
library(pROC)

roc_curve <- roc(student$pass[test], pred)
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
auc(roc_curve)



##
#3 - Linear Discriminant Analiysis
##


# 3.a - Linear Discriminant Model
if (!requireNamespace("MASS", quietly = TRUE)) {install.packages("MASS")}
library(MASS)

lda_model <- lda(pass ~ . -math.score -reading.score -writing.score, 
                 data = student, subset = train)


# 3.b - Evaluation of predictor vectors

# Confusion
lda_predictions <- predict(lda_model, newdata = student[-train,])
confusion_matrix_ <- table(Predicted = lda_predictions$class, Actual = student$pass[-train])
print(confusion_matrix_)

# Accuracy
test <- setdiff(1:nrow(student), train)
lda_pred <- predict(lda_model, newdata = student[test, ])
lda_class <- lda_pred$class
lda_accuracy <- mean(lda_class == student$pass[test])
cat("LDA Model Accuracy:", lda_accuracy, "\n")

# Precision
precision_ <- confusion_matrix_[2,2] / sum(confusion_matrix_[2,])
precision_

# Recall
recall_ <- confusion_matrix_[2,2] / sum(confusion_matrix_[,2])
recall_

# F1
F1_score_ <- 2 * (precision_ * recall_) / (precision_ + recall_)
F1_score_

# Specificity
specificity <- confusion_matrix_[1,1] / sum(confusion_matrix_[,1])
specificity

# Misclassification rate
misclassification_rate_ <- mean(lda_predictions$class != student$pass[-train])
cat("Misclassification Rate:", misclassification_rate_, "\n")

# False positive rate
false_positive_rate <- confusion_matrix_[2,1] / sum(confusion_matrix_[,1])
cat("False Positive Rate:", false_positive_rate, "\n")

# False negative rate
false_negative_rate <- confusion_matrix_[1,2] / sum(confusion_matrix_[,2])
cat("False Negative Rate:", false_negative_rate, "\n")

# ROC - AUC
if (!requireNamespace("pROC", quietly = TRUE)) {install.packages("pROC")}
library(pROC)

colnames(lda_predictions$posterior)
lda_prob <- lda_predictions$posterior[, "1"]
roc_curve_ <- roc(student$pass[test], lda_prob)
plot(roc_curve_, main = "ROC Curve - LDA", col = "blue", lwd = 2)
auc_score <- auc(roc_curve_)
cat("AUC:", auc_score, "\n")

# 3.c - New model

student <- student %>%
  mutate(grade = case_when(
    math.score >= 90 ~ "HighGrade",
    math.score >= 75 & math.score < 90 ~ "MediumGrade",
    math.score >= 50 & math.score < 75 ~ "LowGrade",
    TRUE ~ "Fail"
  ))

lda_model1 <- lda(grade ~ . -math.score -reading.score -writing.score -pass, 
                  data = student, subset = train)

# 3.d - Evaluation of predictor vectors & visulizaition

# Confusion
lda_predictions1 <- predict(lda_model1, newdata = student[-train,])
confusion_matrix1 <- table(Predicted = lda_predictions$class, Actual = student$grade[-train])
print(confusion_matrix1)

# Accuracy
test1 <- setdiff(1:nrow(student), train)
lda_pred1 <- predict(lda_model1, newdata = student[test, ])
lda_class1 <- lda_pred1$class
lda_accuracy1 <- mean(lda_class1 == student$grade[-train])
cat("LDA Model Accuracy:", lda_accuracy1, "\n")

# Precision
precision1 <- confusion_matrix1[2,2] / sum(confusion_matrix1[2,])
precision1

# Recall
recall1 <- confusion_matrix1[2,2] / sum(confusion_matrix1[,2])
recall1

# F1
F1_score1 <- 2 * (precision1 * recall1) / (precision1 + recall1)
F1_score1

# Specificity
specificity <- confusion_matrix1[1,1] / sum(confusion_matrix1[,1])
specificity

# Misclassification rate
misclassification_rate1 <- mean(lda_predictions1$class != student$grade[-train])
cat("Misclassification Rate:", misclassification_rate1, "\n")

# False positive rate
false_positive_rate1 <- confusion_matrix1[2,1] / sum(confusion_matrix1[,1])
cat("False Positive Rate:", false_positive_rate1, "\n")

# False negative rate
false_negative_rate1 <- confusion_matrix1[1,2] / sum(confusion_matrix1[,2])
cat("False Negative Rate:", false_negative_rate1, "\n")

# ROC - AUC
if (!requireNamespace("pROC", quietly = TRUE)) {install.packages("pROC")}
library(pROC)

colnames(lda_predictions1$posterior)
lda_prob1 <- lda_predictions1$posterior[, "HighGrade"]
roc_curve1 <- roc(response = student$grade[test1] == "HighGrade", predictor = lda_prob1)
plot(roc_curve1, main = "ROC Curve - HighGrade vs Rest", col = "blue", lwd = 2)
auc(roc_curve1)

lda_prob2 <- lda_predictions1$posterior[, "Fail"]
roc_curve2 <- roc(response = student$grade[test1] == "Fail", predictor = lda_prob2)
plot(roc_curve2, main = "ROC Curve - Fail vs Rest", col = "blue", lwd = 2)
auc(roc_curve2)

lda_prob3 <- lda_predictions1$posterior[, "LowGrade"]
roc_curve3 <- roc(response = student$grade[test1] == "LowGrade", predictor = lda_prob3)
plot(roc_curve3, main = "ROC Curve - LowGrade vs Rest", col = "blue", lwd = 2)
auc(roc_curve3)

lda_prob4 <- lda_predictions1$posterior[, "MediumGrade"]
roc_curve4 <- roc(response = student$grade[test1] == "MediumGrade", predictor = lda_prob4)
plot(roc_curve4, main = "ROC Curve - MediumGrade vs Rest", col = "blue", lwd = 2)
auc(roc_curve4)

# Dataframe
plot_data <- data.frame(label = student$grade[-train], 
                        LD1 = lda_predictions$x[,1])


# One discriminant
ggplot(plot_data, aes(x = LD1, colour = label)) + 
  geom_density() + 
  labs(title = "LDA Projection (LD1)", x = "Linear Discriminant 1", y = "Density") +
  theme_minimal()
